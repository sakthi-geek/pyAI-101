{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "\n",
    "    def __init__(self, n_inputs, learning_rate=0.01):\n",
    "        \n",
    "        self.lr = learning_rate\n",
    "        self.weights = np.random.randn(n_inputs) * self.lr\n",
    "        self.bias = np.zeros(1)\n",
    "        self.inputs = None\n",
    "        # For storing gradients\n",
    "        self.grad_weights = None\n",
    "        self.grad_bias = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        self.grad_inputs = np.dot(grad_output, self.weights.T)      # Gradient with respect to the input\n",
    "        # Compute gradients of weights and bias\n",
    "        self.grad_weights = np.dot(self.inputs.T, grad_output)      # Gradient with respect to the weights\n",
    "        self.grad_bias = np.sum(grad_output, axis=0, keepdims=True) # Gradient with respect to the bias\n",
    "\n",
    "        # Here, instead of directly updating weights, store gradients for an optimizer to use\n",
    "        return self.grad_inputs, self.grad_weights, self.grad_bias\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Return a string representation of the neuron.\n",
    "        \"\"\"\n",
    "        return f\"Neuron(weight={self.weights}, bias={self.bias}, grad_weights={self.grad_weights}, grad_bias={self.grad_bias}, learning_rate={self.learning_rate})\"\n",
    "\n",
    "\n",
    "\n",
    "class PerceptronFromNeuron:\n",
    "    def __init__(self, n_inputs, learning_rate=0.01):\n",
    "        # The perceptron is a single neuron in this context\n",
    "        self.neuron = Neuron(n_inputs, learning_rate=learning_rate)\n",
    "    \n",
    "    def activation(self, x):\n",
    "        # Step function\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Compute the weighted sum (linear output)\n",
    "        linear_output = np.dot(inputs, self.neuron.weights) + self.neuron.bias\n",
    "        # Apply the activation function\n",
    "        return self.activation(linear_output)\n",
    "        \n",
    "    def train(self, X, y, epochs):\n",
    "        for _ in range(epochs):\n",
    "            for input, label in zip(X, y):\n",
    "                # Compute the output of the perceptron\n",
    "                output = self.forward(input)\n",
    "                # Calculate the error\n",
    "                error = label - output\n",
    "                # Update the weights and bias\n",
    "                self.neuron.weights += self.neuron.learning_rate * error * input\n",
    "                self.neuron.bias += self.neuron.learning_rate * error\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "\n",
    "    def __init__(self, input_dim, lr=0.01, n_iters=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.zeros(input_dim)\n",
    "        self.bias = 0.0\n",
    "\n",
    "    def activation(self, x):\n",
    "        \"\"\" Step function \"\"\"\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Compute the linear output of the perceptron.\"\"\"\n",
    "        # Compute the weighted sum (linear output)\n",
    "        linear_output = np.dot(x, self.weights) + self.bias\n",
    "        # Apply the activation function\n",
    "        return self.activation(linear_output)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"Train the perceptron model on the training data.\"\"\"        \n",
    "        # Training loop\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                y_pred = self.forward(x_i)\n",
    "                # Calculate the error\n",
    "                error = y[idx] - y_pred\n",
    "                # Update weights and bias\n",
    "                self.weights += self.lr * error * x_i\n",
    "                self.bias += self.lr * error\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the class labels for the input data.\"\"\"\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "[0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# AND function\n",
    "X_and = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y_and = np.array([0, 0, 0, 1])\n",
    "\n",
    "# OR function\n",
    "X_or = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_or = np.array([0, 1, 1, 1])\n",
    "\n",
    "print(X_or)\n",
    "print(y_or)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "\n",
    "n_iters = 10000      # number of iterations\n",
    "lr = 0.01           # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on OR function: n_iters = 10000, lr = 0.01\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# Training the model on OR function\n",
    "print(\"Training on OR function: n_iters = {}, lr = {}\".format(n_iters, lr))\n",
    "custom_or_perceptron = Perceptron(lr=lr, n_iters=n_iters)\n",
    "custom_or_perceptron.fit(X_or, y_or)\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the OR perceptron\n",
      "[0 1 1 1]\n",
      "labels       :  [0 1 1 1]\n",
      "predictions  :  [0 1 1 1]\n",
      "[0 0] -> 0 -> 0\n",
      "[0 1] -> 1 -> 1\n",
      "[1 0] -> 1 -> 1\n",
      "[1 1] -> 1 -> 1\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "print(\"Testing the OR perceptron\")\n",
    "predictions = custom_or_perceptron.predict(X_or)\n",
    "print(predictions)\n",
    "print(\"labels       : \", y_or)\n",
    "print(\"predictions  : \", predictions.round())\n",
    "for input in X_or:\n",
    "    pred_out = custom_or_perceptron.predict(input)\n",
    "    print(f\"{input} -> {pred_out} -> {pred_out.round()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron using Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class PerceptronTorch(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(PerceptronTorch, self).__init__()\n",
    "        self.fc = torch.nn.Linear(input_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.fc(x))        # sigmoid function as activation function\n",
    "    \n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            return self.forward(x)\n",
    "    \n",
    "    def fit(self, X, y, lr=0.01, n_iters=1000, optimizer=None, loss_criterion=None):\n",
    "        if optimizer is None:\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n",
    "        if loss_criterion is None:\n",
    "            loss_criterion = torch.nn.BCELoss()\n",
    "\n",
    "        for _ in range(n_iters):\n",
    "            optimizer.zero_grad()\n",
    "            output = self.forward(X)\n",
    "            loss = loss_criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, name=\"sgd\", lr=0.01):\n",
    "    if name == \"sgd\":\n",
    "        return torch.optim.SGD(model.parameters(), lr=lr)           # Stochastic Gradient Descent\n",
    "    elif name == \"adam\":\n",
    "        return torch.optim.Adam(model.parameters(), lr=lr)          # Adam optimizer\n",
    "    elif name == \"rmsprop\":\n",
    "        return torch.optim.RMSprop(model.parameters(), lr=lr)       # RMSprop optimizer\n",
    "    elif name == \"adagrad\":\n",
    "        return torch.optim.Adagrad(model.parameters(), lr=lr)       # Adagrad optimizer\n",
    "    elif name == \"adadelta\":\n",
    "        return torch.optim.Adadelta(model.parameters(), lr=lr)      # Adadelta optimizer\n",
    "    elif name == \"adamw\":\n",
    "        return torch.optim.AdamW(model.parameters(), lr=lr)         # AdamW optimizer\n",
    "    elif name == \"adamax\":\n",
    "        return torch.optim.Adamax(model.parameters(), lr=lr)        # Adamax optimizer\n",
    "    elif name == \"asgd\":\n",
    "        return torch.optim.ASGD(model.parameters(), lr=lr)          # ASGD optimizer\n",
    "    elif name == \"rprop\":\n",
    "        return torch.optim.Rprop(model.parameters(), lr=lr)         # Rprop optimizer\n",
    "    elif name == \"lbfgs\":\n",
    "        return torch.optim.LBFGS(model.parameters(), lr=lr)         # LBFGS optimizer\n",
    "    else:\n",
    "        return torch.optim.SGD(model.parameters(), lr=lr)           # Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_criterion = torch.nn.BCELoss()              # Binary Cross-Entropy Loss for binary classification\n",
    "# loss_criterion = torch.nn.MSELoss()            # Mean Squared Error Loss for regression\n",
    "# loss_criterion = torch.nn.CrossEntropyLoss()   # Cross-Entropy Loss for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR function data\n",
    "X_or = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y_or = torch.tensor([0, 1, 1, 1], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# AND function data\n",
    "X_and = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y_and = torch.tensor([0, 0, 0, 1], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "print(X_or)\n",
    "print(y_or)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "\n",
    "n_iters = 10000      # number of iterations\n",
    "lr = 0.01           # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on OR function: n_iters = 10000, lr = 0.01\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# Training the model on OR function\n",
    "print(\"Training on OR function: n_iters = {}, lr = {}\".format(n_iters, lr))\n",
    "or_perceptron = PerceptronTorch(input_dim=2)\n",
    "or_optimizer = create_optimizer(or_perceptron, name=\"sgd\", lr=0.01)\n",
    "loss_criterion = torch.nn.BCELoss()\n",
    "or_perceptron.fit(X_or, y_or, lr, n_iters, optimizer=or_optimizer, loss_criterion=loss_criterion)\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing trained OR perceptron:\n",
      "tensor([[0.1841],\n",
      "        [0.9262],\n",
      "        [0.9308],\n",
      "        [0.9987]])\n",
      "labels       :  tensor([0., 1., 1., 1.])\n",
      "predictions  :  tensor([0., 1., 1., 1.])\n",
      "tensor([0., 0.]) -> tensor([0.1841]) -> [0.]\n",
      "tensor([0., 1.]) -> tensor([0.9262]) -> [1.]\n",
      "tensor([1., 0.]) -> tensor([0.9308]) -> [1.]\n",
      "tensor([1., 1.]) -> tensor([0.9987]) -> [1.]\n"
     ]
    }
   ],
   "source": [
    "# Testing \n",
    "print(\"Testing the OR perceptron:\")\n",
    "predictions = or_perceptron.predict(X_or)\n",
    "print(predictions)\n",
    "print(\"labels       : \", y_or.view(-1))\n",
    "print(\"predictions  : \", predictions.round().view(-1))\n",
    "for input in X_or:\n",
    "    pred_out = or_perceptron.predict(input)\n",
    "    print(f\"{input} -> {pred_out} -> {pred_out.round().view(-1).numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on AND function:\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# Training the model on AND function\n",
    "print(\"Training on AND function:\")\n",
    "and_perceptron = PerceptronTorch(input_dim=2)\n",
    "and_optimizer = create_optimizer(and_perceptron, name=\"sgd\", lr=0.01)\n",
    "loss_criterion = torch.nn.BCELoss()\n",
    "and_perceptron.fit(X_and, y_and, lr, n_iters, optimizer=and_optimizer, loss_criterion=loss_criterion)\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing trained perceptron:\n",
      "tensor([[0.0076],\n",
      "        [0.1469],\n",
      "        [0.1450],\n",
      "        [0.7919]])\n",
      "labels       :  tensor([0., 0., 0., 1.])\n",
      "predictions  :  tensor([0., 0., 0., 1.])\n",
      "tensor([0., 0.]) -> tensor([0.0076]) -> [0.]\n",
      "tensor([0., 1.]) -> tensor([0.1469]) -> [0.]\n",
      "tensor([1., 0.]) -> tensor([0.1450]) -> [0.]\n",
      "tensor([1., 1.]) -> tensor([0.7919]) -> [1.]\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "print(\"Testing the AND perceptron:\")\n",
    "predictions = and_perceptron.predict(X_and)\n",
    "print(predictions)\n",
    "print(\"labels       : \", y_and.view(-1))\n",
    "print(\"predictions  : \", predictions.round().view(-1))\n",
    "for input in X_and:\n",
    "    pred_out = and_perceptron.predict(input)\n",
    "    print(f\"{input} -> {pred_out} -> {pred_out.round().view(-1).numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39ai101_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
